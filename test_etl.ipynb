{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1. Create 3 directories song_data , log_data and output_files by typing : mkdir output_files\n",
    "# 2. Unzip the the song data and log data to the above file : \n",
    "#      - unzip /home/workspace/data/song-data.zip -d /home/workspace/song_data    \n",
    "#      - unzip /home/workspace/data/log-data.zip -d /home/workspace/log_data\n",
    "# 3. Run the etl script: spark-submit etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format \n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['KEYS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['KEYS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_song = \"./song_data\"\n",
    "input_log = \"./\"\n",
    "output_data = \"./output_files\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_song = spark.read.json(os.path.join(input_song, 'song_data/*/*/*/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_song.createOrReplaceTempView(\"staging_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    " songs_df= spark.sql (\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            song_id     ,\n",
    "            title       ,\n",
    "            artist_id   ,\n",
    "            duration    ,\n",
    "            year\n",
    "        FROM staging_songs\n",
    "    \"\"\").collect()\n",
    "df1 = spark.createDataFrame(songs_df)\n",
    "df1.write.partitionBy(\"year\", \"artist_id\").parquet(os.path.join(output_data, 'songs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_df= spark.sql (\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            artist_id          AS artist_id ,\n",
    "            artist_name        AS name      ,\n",
    "            artist_location    AS location  ,\n",
    "            artist_latitude    AS latitude  ,\n",
    "            artist_longitude   AS longtitude\n",
    "        FROM staging_songs  \n",
    "    \"\"\").collect()\n",
    "df2 = spark.createDataFrame(artists_df)\n",
    "df2.write.parquet(os.path.join(output_data, 'artists'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log = spark.read.json(os.path.join(input_log, \"log_data/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda x: datetime.fromtimestamp((x/1000.0)), TimestampType())\n",
    "df_log = df_log.withColumn('timestamp', get_timestamp(df_log.ts))\n",
    "df_log = df_log.withColumn('date', to_date(\"timestamp\"))\n",
    "df_log = df_log.withColumn('start_time', date_format(\"timestamp\", 'HH:mm:ss'))\n",
    "df_log = df_log.withColumn('hour', hour(\"timestamp\"))\n",
    "df_log = df_log.withColumn('day', dayofmonth(\"date\"))\n",
    "df_log = df_log.withColumn('week', weekofyear(\"date\"))\n",
    "df_log = df_log.withColumn('month', month(\"date\"))\n",
    "df_log = df_log.withColumn('year', year(\"date\"))\n",
    "df_log = df_log.withColumn('weekday', date_format('date', 'E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log.createOrReplaceTempView(\"staging_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_df = spark.sql (\"\"\"\n",
    "        SELECT DISTINCT\n",
    "                userid       AS user_id    ,\n",
    "                firstname    AS first_name ,\n",
    "                lastname     AS last_name  ,\n",
    "                gender       AS gender     ,  \n",
    "                level        AS level\n",
    "        FROM staging_events\n",
    "        WHERE page = 'NextSong'\n",
    "    \"\"\").collect()\n",
    "df3 = spark.createDataFrame(user_df)\n",
    "df3.write.mode('overwrite').parquet(os.path.join(output_data, 'users'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_df= spark.sql (\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            start_time,\n",
    "            hour      , \n",
    "            day       ,\n",
    "            week      , \n",
    "            month     ,\n",
    "            year      , \n",
    "            weekday\n",
    "        FROM staging_events\n",
    "    \"\"\").collect()\n",
    "df4 = spark.createDataFrame(time_df)\n",
    "df4.write.partitionBy(\"year\", \"month\").mode('overwrite').parquet(os.path.join(output_data, 'time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_df = spark.sql (\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            staging_events.userId       AS user_id   ,\n",
    "            staging_songs.song_id       AS song_id   ,\n",
    "            staging_songs.artist_id     AS artist_id ,\n",
    "            staging_events.start_time   AS startime  ,\n",
    "            staging_events.level        AS level     ,\n",
    "            staging_events.sessionId    AS session_id,\n",
    "            staging_events.location     AS location  ,\n",
    "            staging_events.userAgent    AS user_agent,\n",
    "            staging_events.year         AS year      ,\n",
    "            staging_events.month        AS month\n",
    "\n",
    "        FROM staging_events          staging_events\n",
    "\n",
    "        JOIN staging_songs           staging_songs\n",
    "        ON   staging_events.artist = staging_songs.artist_name\n",
    "        AND  staging_events.song   = staging_songs.title\n",
    "        AND  staging_events.length = staging_songs.duration\n",
    "\n",
    "        WHERE staging_events.page  = 'NextSong'\n",
    "    \"\"\").collect()\n",
    "df5 = spark.createDataFrame(songplays_df)\n",
    "df5.write.partitionBy(\"year\", \"month\").mode('overwrite').parquet(os.path.join(output_data, 'songplays'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
